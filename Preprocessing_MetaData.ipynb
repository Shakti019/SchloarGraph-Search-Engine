{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mount Google Drive\n",
    "Mounts Google Drive to access the raw dataset collected in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29900,
     "status": "ok",
     "timestamp": 1766408885770,
     "user": {
      "displayName": "Shakti Singh",
      "userId": "15110137530207228295"
     },
     "user_tz": -330
    },
    "id": "wVR-B3wME-l_",
    "outputId": "76f98013-8332-47e1-8622-760e0d61b49b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect Raw Data\n",
    "This cell defines a helper function `quick_inspect_jsonl` to read the raw JSONL file. It counts the total number of lines (papers) and prints the first paper object to verify the data structure before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8835,
     "status": "ok",
     "timestamp": 1766408899204,
     "user": {
      "displayName": "Shakti Singh",
      "userId": "15110137530207228295"
     },
     "user_tz": -330
    },
    "id": "RpAheOWtFKPx",
    "outputId": "f1ed68b8-9e4f-4357-b868-a0719b2fbff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 331600\n",
      "\n",
      "First paper sample:\n",
      "\n",
      "{\n",
      "  \"openalex_id\": \"https://openalex.org/W1803273808\",\n",
      "  \"doi\": \"https://doi.org/10.4135/9781036235611\",\n",
      "  \"title\": \"The Coding Manual for Qualitative Researchers\",\n",
      "  \"abstract\": null,\n",
      "  \"publication_year\": 2025,\n",
      "  \"publication_date\": \"2025-01-01\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"author_id\": \"https://openalex.org/A5058231379\",\n",
      "      \"name\": \"Johnny Salda\\u00f1a\"\n",
      "    }\n",
      "  ],\n",
      "  \"concepts\": [\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C179518139\",\n",
      "      \"name\": \"Coding (social sciences)\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C2780031656\",\n",
      "      \"name\": \"Glossary\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C73231260\",\n",
      "      \"name\": \"Tunstall coding\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C130811719\",\n",
      "      \"name\": \"Shannon\\u2013Fano coding\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C60603091\",\n",
      "      \"name\": \"Variable-length code\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C41008148\",\n",
      "      \"name\": \"Computer science\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C135534801\",\n",
      "      \"name\": \"Context-adaptive variable-length coding\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C175732694\",\n",
      "      \"name\": \"Context-adaptive binary arithmetic coding\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C33923547\",\n",
      "      \"name\": \"Mathematics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C41895202\",\n",
      "      \"name\": \"Linguistics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C105795698\",\n",
      "      \"name\": \"Statistics\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/C138885662\",\n",
      "      \"name\": \"Philosophy\"\n",
      "    }\n",
      "  ],\n",
      "  \"venue\": null,\n",
      "  \"citation_count\": 17798,\n",
      "  \"is_open_access\": false,\n",
      "  \"oa_status\": \"closed\",\n",
      "  \"url\": \"https://openalex.org/W1803273808\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "FILE_PATH = \"/content/drive/MyDrive/OpenAlex_CS_2025/openalex_cs_2025.jsonl\"\n",
    "\n",
    "def quick_inspect_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first = None\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                if first is None:\n",
    "                    first = json.loads(line)\n",
    "                count += 1\n",
    "        return count, first\n",
    "\n",
    "total, sample = quick_inspect_jsonl(FILE_PATH)\n",
    "\n",
    "print(\"Total papers:\", total)\n",
    "print(\"\\nFirst paper sample:\\n\")\n",
    "print(json.dumps(sample, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup Paths\n",
    "Defines the input path (raw data) and the output directory where the processed, domain-partitioned files will be stored. It ensures the output directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1GLZnHCF_G-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_FILE = \"/content/drive/MyDrive/OpenAlex_CS_2025/openalex_cs_2025.jsonl\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/OpenAlex_CS_2025_Data/processed\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Domain Keywords\n",
    "This is a crucial configuration step. We define a dictionary `DOMAIN_KEYWORDS` that maps specific keywords (e.g., 'neural network', 'text mining') to their respective domains (AI, ML, DL, NLP, CV, RL). This dictionary drives the domain detection logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9w2oqErXGAjC"
   },
   "outputs": [],
   "source": [
    "DOMAIN_KEYWORDS = {\n",
    "    \"ai\": {\"artificial intelligence\"},\n",
    "    \"ml\": {\"machine learning\"},\n",
    "    \"dl\": {\"deep learning\", \"neural network\"},\n",
    "    \"nlp\": {\"natural language processing\", \"speech recognition\", \"text mining\"},\n",
    "    \"cv\": {\"computer vision\", \"image processing\", \"object detection\"},\n",
    "    \"rl\": {\"reinforcement learning\"},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Cleaning & Detection Functions\n",
    "We define two helper functions:\n",
    "*   `clean_nulls`: Recursively removes empty fields (None, empty strings, empty lists) from the paper object to reduce storage size and noise.\n",
    "*   `detect_domains`: Iterates through a paper's concepts and checks them against the `DOMAIN_KEYWORDS` to identify which domain(s) the paper belongs to. A paper can belong to multiple domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ib87fKpGExw"
   },
   "outputs": [],
   "source": [
    "def clean_nulls(obj):\n",
    "    \"\"\"Remove null / empty fields recursively\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_nulls(v) for k, v in obj.items() if v not in [None, \"\", [], {}]}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_nulls(v) for v in obj if v not in [None, \"\", [], {}]]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def detect_domains(concepts):\n",
    "    \"\"\"Detect CS subdomains from concept names\"\"\"\n",
    "    found = set()\n",
    "    for c in concepts:\n",
    "        name = c.get(\"name\", \"\").lower()\n",
    "        for domain, keywords in DOMAIN_KEYWORDS.items():\n",
    "            if any(k in name for k in keywords):\n",
    "                found.add(domain)\n",
    "    return found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Process and Partition Data\n",
    "This is the main processing loop:\n",
    "1.  Opens output files for each domain (plus 'other_cs').\n",
    "2.  Iterates through the raw dataset line by line.\n",
    "3.  Cleans each paper object.\n",
    "4.  Detects the domain(s) based on concepts.\n",
    "5.  Writes the paper to the corresponding domain file(s). If no specific domain is found, it goes to 'other_cs'.\n",
    "6.  Prints the final counts for each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65570,
     "status": "ok",
     "timestamp": 1766409024213,
     "user": {
      "displayName": "Shakti Singh",
      "userId": "15110137530207228295"
     },
     "user_tz": -330
    },
    "id": "1dk5kAUhGRwF",
    "outputId": "2169508f-d3a8-4f08-ffa2-cc851e22a240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CS 2025 papers: 331600it [01:05, 5049.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DONE\n",
      "{\n",
      "  \"ai\": 127537,\n",
      "  \"ml\": 36967,\n",
      "  \"dl\": 18751,\n",
      "  \"nlp\": 10611,\n",
      "  \"cv\": 19221,\n",
      "  \"rl\": 2963,\n",
      "  \"other_cs\": 186251\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_files = {\n",
    "    domain: open(os.path.join(OUTPUT_DIR, f\"{domain}.jsonl\"), \"a\", encoding=\"utf-8\")\n",
    "    for domain in DOMAIN_KEYWORDS\n",
    "}\n",
    "output_files[\"other_cs\"] = open(os.path.join(OUTPUT_DIR, \"other_cs.jsonl\"), \"a\", encoding=\"utf-8\")\n",
    "\n",
    "counts = {k: 0 for k in output_files}\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in tqdm(infile, desc=\"Processing CS 2025 papers\"):\n",
    "        paper = json.loads(line)\n",
    "\n",
    "        # Clean nulls\n",
    "        paper = clean_nulls(paper)\n",
    "\n",
    "        concepts = paper.get(\"concepts\", [])\n",
    "        domains = detect_domains(concepts)\n",
    "\n",
    "        if domains:\n",
    "            for d in domains:\n",
    "                output_files[d].write(json.dumps(paper) + \"\\n\")\n",
    "                counts[d] += 1\n",
    "        else:\n",
    "            output_files[\"other_cs\"].write(json.dumps(paper) + \"\\n\")\n",
    "            counts[\"other_cs\"] += 1\n",
    "\n",
    "# Close files\n",
    "for f in output_files.values():\n",
    "    f.close()\n",
    "\n",
    "print(\"✅ DONE\")\n",
    "print(json.dumps(counts, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOex4yQUy9KWW1uowEn0rO1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
